{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NL28cpJKUxnE"
      },
      "source": [
        "# This uses all four datasets"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF4DKdY2gO_Z"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy1maEvif5p8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 270
        },
        "outputId": "df049a96-13e1-4056-ab2e-682624d15e80"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-128da2137812>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmagic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'matplotlib inline'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     79\u001b[0m     \u001b[0;31m# it and importing it first would fail if the OpenMP dll cannot be found.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m_distributor_init\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 81\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0m__check_build\u001b[0m  \u001b[0;31m# noqa: F401\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     82\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mbase\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mclone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_show_versions\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mshow_versions\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/sklearn/__check_build/__init__.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m     \u001b[0;32mfrom\u001b[0m \u001b[0;34m.\u001b[0m\u001b[0m_check_build\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcheck_build\u001b[0m  \u001b[0;31m# noqa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m \u001b[0;32mexcept\u001b[0m \u001b[0mImportError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m     \u001b[0mraise_build_error\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.6/importlib/_bootstrap.py\u001b[0m in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqYe6Xlzb8oq",
        "outputId": "be6b60bd-060e-4038-adee-0c32f4d52144"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GF9FW1dgWeR"
      },
      "source": [
        "### Check GPU Avaliability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYs0N24uggfO",
        "outputId": "d559188b-04f4-41e8-eb3a-4211c7abbf06"
      },
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJH59-U7wyDv"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QkrOJXdvHMdH"
      },
      "source": [
        "#### Loading Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PavBkWpxfUo2"
      },
      "source": [
        "dataset1 = pd.read_csv('/content/drive/My Drive/184A/finalProj/data/processed.cleveland.txt', header = None)\n",
        "dataset1.columns = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num']\n",
        "dataset1 = dataset1.replace('?', np.nan)\n",
        "dataset1 = dataset1.astype({'age': 'float64','sex': 'category','cp': 'category','trestbps': 'float64','chol': 'float64', 'fbs': 'category', 'restecg': 'category', 'thalach': 'float64', 'exang': 'category', 'oldpeak': 'float64', 'slope': 'category', 'ca': 'category', 'thal': 'category', 'num': 'category'})\n",
        "\n",
        "dataset2 = pd.read_csv('/content/drive/My Drive/184A/finalProj/data/reprocessed.hungarian.txt', sep=\" \", header = None)\n",
        "dataset2.columns = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num']\n",
        "dataset2 = dataset2.replace(-9, np.nan)\n",
        "dataset2['ca'] = dataset2['ca'].replace(9, np.nan)\n",
        "dataset2 = dataset2.astype({'age': 'float64','sex': 'category','cp': 'category','trestbps': 'float64','chol': 'float64', 'fbs': 'category', 'restecg': 'category', 'thalach': 'float64', 'exang': 'category', 'oldpeak': 'float64', 'slope': 'category', 'ca': 'category', 'thal': 'category', 'num': 'category'})\n",
        "\n",
        "dataset3 = pd.read_csv('/content/drive/My Drive/184A/finalProj/data/processed.switzerland.txt', header = None)\n",
        "dataset3.columns = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num']\n",
        "dataset3 = dataset3.replace('?', np.nan)\n",
        "dataset3['chol'] = dataset3['chol'].replace(0,np.nan)\n",
        "dataset3 = dataset3.astype({'age': 'float64','sex': 'category','cp': 'category','trestbps': 'float64','chol': 'float64', 'fbs': 'category', 'restecg': 'category', 'thalach': 'float64', 'exang': 'category', 'oldpeak': 'float64', 'slope': 'category', 'ca': 'category', 'thal': 'category', 'num': 'category'})\n",
        "\n",
        "dataset4 = pd.read_csv('/content/drive/My Drive/184A/finalProj/data/processed.va.txt', header = None)\n",
        "dataset4.columns = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num']\n",
        "dataset4 = dataset4.replace('?', np.nan)\n",
        "dataset4['chol'] = dataset4['chol'].replace('0',np.nan)\n",
        "dataset4['trestbps'] = dataset4['trestbps'].replace('0',np.nan)\n",
        "dataset4 = dataset4.astype({'age': 'float64','sex': 'category','cp': 'category','trestbps': 'float64','chol': 'float64', 'fbs': 'category', 'restecg': 'category', 'thalach': 'float64', 'exang': 'category', 'oldpeak': 'float64', 'slope': 'category', 'ca': 'category', 'thal': 'category', 'num': 'category'})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gbf0AMnWHOjz"
      },
      "source": [
        "#### Creating Datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f4wKnI2tGS_t",
        "outputId": "100f03a4-8152-4479-8729-45bea8fd168b"
      },
      "source": [
        "datasetAll = pd.concat([dataset1, dataset2, dataset3, dataset4], ignore_index=True, sort=False)\n",
        "\n",
        "#top 6 features - oldpeak, chol, thalach, age, trestbps, ca\n",
        "#middle - slope\n",
        "#bottom - restecg, cp, thal, fbs, exang, sex (lowest)\n",
        "\n",
        "#drop ca, slope, thal\n",
        "#(558, 11)\n",
        "datasetV1 = datasetAll.drop(['slope', 'ca', 'thal'], axis = 1)\n",
        "datasetV1 = datasetV1.dropna()\n",
        "datasetV1 = datasetV1.reset_index(drop=True)\n",
        "\n",
        "#drop ca, thal (will lose 1/3 of data since slope is 1/3 nan)\n",
        "#(392, 12)\n",
        "datasetV2 = datasetAll.drop(['ca', 'thal'], axis = 1)\n",
        "datasetV2 = datasetV2.dropna()\n",
        "datasetV2 = datasetV2.reset_index(drop=True)\n",
        "\n",
        "#drop thal (will still lose majority of data since ca had 2/3 nan)\n",
        "#(297, 13)\n",
        "datasetV3 = datasetAll.drop(['thal'], axis = 1)\n",
        "datasetV3 = datasetV3.dropna()\n",
        "datasetV3 = datasetV3.reset_index(drop=True)\n",
        "\n",
        "print(datasetV1.shape)\n",
        "print(datasetV1['num'].value_counts())\n",
        "print()\n",
        "print(datasetV2.shape)\n",
        "print(datasetV2['num'].value_counts())\n",
        "print()\n",
        "print(datasetV3.shape)\n",
        "print(datasetV3['num'].value_counts())\n",
        "\n",
        "#pd.set_option('display.max_rows', None)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "#print(datasetV1.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(661, 11)\n",
            "0.0    347\n",
            "1.0    119\n",
            "3.0     82\n",
            "2.0     80\n",
            "4.0     33\n",
            "Name: num, dtype: int64\n",
            "\n",
            "(462, 12)\n",
            "0.0    201\n",
            "1.0     94\n",
            "3.0     73\n",
            "2.0     64\n",
            "4.0     30\n",
            "Name: num, dtype: int64\n",
            "\n",
            "(301, 13)\n",
            "0.0    161\n",
            "1.0     56\n",
            "2.0     36\n",
            "3.0     35\n",
            "4.0     13\n",
            "Name: num, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EaCKsg4rHSmK"
      },
      "source": [
        "#### Scaling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x3HZGK50HUYj"
      },
      "source": [
        "from sklearn.preprocessing import MinMaxScaler\n",
        "scaler = MinMaxScaler()\n",
        "datasetV1.iloc[:,0:-1] = scaler.fit_transform(datasetV1.iloc[:,0:-1])\n",
        "datasetV2.iloc[:,0:-1] = scaler.fit_transform(datasetV2.iloc[:,0:-1])\n",
        "datasetV3.iloc[:,0:-1] = scaler.fit_transform(datasetV3.iloc[:,0:-1])"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CS9m016rJRFQ"
      },
      "source": [
        "### Choosing dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nO1HZJEcJQK6",
        "outputId": "517df941-93a7-426f-b72c-087b1e5eb191"
      },
      "source": [
        "theDataset = datasetV1 #10 inputs w/o ca, slope, thal\n",
        "#theDataset = datasetV2 #11 inputs w/o ca, thal\n",
        "#theDataset = datasetV3 #12 inputs w/o thal\n",
        "inputNum = theDataset.shape[1] -1\n",
        "print(theDataset.shape)\n",
        "print(theDataset['num'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(661, 11)\n",
            "0.0    347\n",
            "1.0    119\n",
            "3.0     82\n",
            "2.0     80\n",
            "4.0     33\n",
            "Name: num, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upt4S2nUIRV3"
      },
      "source": [
        "#### To turn it into binary classifier"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rHOgcQApIGz9"
      },
      "source": [
        "#def changeTarget(value):\n",
        "#  return 1 if value > 0 else 0\n",
        "#theDataset['num'] = theDataset['num'].apply(changeTarget) #to make it just 0 and 1's"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egeCL0gaQq4a"
      },
      "source": [
        "### Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XAaYvauwSic"
      },
      "source": [
        "# Hyper-parameters\n",
        "input_size = inputNum\n",
        "hidden_size = inputNum-1\n",
        "num_classes = (theDataset['num'].value_counts()).size\n",
        "\n",
        "num_epochs = 100\n",
        "batch_size = 30\n",
        "learning_rate = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pJpyTLqvNekG"
      },
      "source": [
        "### Setting Up Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2ZIXzgrNHj7"
      },
      "source": [
        "#### Seperate"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "20kV2YI5NOPI"
      },
      "source": [
        "#from sklearn.utils import shuffle\n",
        "#np.random.seed(0)\n",
        "#theDataset = shuffle(theDataset)\n",
        "\n",
        "#X = theDataset.iloc[:,0:-1]\n",
        "#y = theDataset.iloc[:,-1]\n",
        "#Xtr, Xva, Ytr, Yva = train_test_split(X, y, test_size=0.25, random_state=2, stratify=y)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yfsA6qhYNbS-"
      },
      "source": [
        "#### Seperate and Over/Undersampling"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T51C73xONa7P",
        "outputId": "1c0a3e7c-911b-4d6c-92fa-f6446492e851"
      },
      "source": [
        "# import warnings filter\n",
        "from warnings import simplefilter\n",
        "# ignore all future warnings\n",
        "simplefilter(action='ignore', category=FutureWarning)\n",
        "\n",
        "from collections import Counter\n",
        "from imblearn.under_sampling import NeighbourhoodCleaningRule, TomekLinks, ClusterCentroids, NearMiss, EditedNearestNeighbours, CondensedNearestNeighbour, OneSidedSelection, RandomUnderSampler\n",
        "from imblearn.over_sampling import SMOTE\n",
        "\n",
        "XtrSamp = Xtr\n",
        "YtrSamp = Ytr\n",
        "\n",
        "print('Original dataset shape %s' % Counter(Ytr))\n",
        "\n",
        "print('\\nOversampling')\n",
        "smote = SMOTE()\n",
        "X_res, y_res = smote.fit_resample(Xtr, Ytr)\n",
        "print('SMOTE shape %s' % Counter(y_res))\n",
        "#Xtr = X_res\n",
        "#Ytr = y_res\n",
        "#print(\"Doing Over\")\n",
        "\n",
        "print('\\nRandom')\n",
        "rus = RandomUnderSampler(sampling_strategy='auto')\n",
        "X_res, y_res = rus.fit_resample(Xtr, Ytr)\n",
        "print('RandomUnderSampler shape %s' % Counter(y_res))\n",
        "\n",
        "print('\\nKeep')\n",
        "cnn = CondensedNearestNeighbour(sampling_strategy='auto')\n",
        "X_res, y_res = cnn.fit_resample(Xtr, Ytr)\n",
        "print('CondensedNearestNeighbour shape %s' % Counter(y_res))\n",
        "\n",
        "nm = NearMiss(sampling_strategy='auto')\n",
        "X_res, y_res = nm.fit_resample(Xtr, Ytr)\n",
        "print('NearMiss shape %s' % Counter(y_res))\n",
        "\n",
        "print('\\nDelete')\n",
        "tl = TomekLinks(sampling_strategy='auto')\n",
        "X_res, y_res = tl.fit_resample(Xtr, Ytr)\n",
        "print('TomekLinks shape %s' % Counter(y_res))\n",
        "\n",
        "enn = EditedNearestNeighbours(sampling_strategy='majority')\n",
        "X_res, y_res = enn.fit_resample(Xtr, Ytr)\n",
        "print('EditedNearestNeighbours shape %s' % Counter(y_res))\n",
        "\n",
        "print('\\nBoth')\n",
        "ncr = NeighbourhoodCleaningRule(sampling_strategy='auto')\n",
        "X_res, y_res = ncr.fit_resample(Xtr, Ytr)\n",
        "print('NeighbourhoodCleaningRule shape %s' % Counter(y_res))\n",
        "\n",
        "oss = OneSidedSelection(sampling_strategy='auto')\n",
        "X_res, y_res = oss.fit_resample(Xtr, Ytr)\n",
        "print('OneSidedSelection shape %s' % Counter(y_res))\n",
        "\n",
        "print()\n",
        "\n",
        "cc = ClusterCentroids(sampling_strategy='auto')\n",
        "X_res, y_res = cc.fit_resample(Xtr, Ytr)\n",
        "print('ClusterCentroids shape %s' % Counter(y_res))\n",
        "\n",
        "#XtrSamp = X_res\n",
        "#YtrSamp = y_res\n",
        "#print(\"Doing Above\")\n",
        "\n",
        "print()\n",
        "dataSamp = pd.DataFrame(np.column_stack((XtrSamp,YtrSamp)))\n",
        "dataSamp.rename({(inputNum): 'num'}, axis=1, inplace=True)\n",
        "dataSamp = shuffle(dataSamp)\n",
        "Xtr = dataSamp.iloc[:,0:-1]\n",
        "Ytr = dataSamp.iloc[:,-1]\n",
        "\n",
        "#pd.set_option('display.max_rows', None)\n",
        "#pd.set_option('display.max_columns', None)\n",
        "#print(dataSamp)\n",
        "print(dataSamp.shape)\n",
        "print(dataSamp['num'].value_counts())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Original dataset shape Counter({0: 260, 1: 235})\n",
            "\n",
            "Oversampling\n",
            "SMOTE shape Counter({1: 260, 0: 260})\n",
            "\n",
            "Random\n",
            "RandomUnderSampler shape Counter({0: 235, 1: 235})\n",
            "\n",
            "Keep\n",
            "CondensedNearestNeighbour shape Counter({1: 235, 0: 90})\n",
            "NearMiss shape Counter({0: 235, 1: 235})\n",
            "\n",
            "Delete\n",
            "TomekLinks shape Counter({1: 235, 0: 231})\n",
            "EditedNearestNeighbours shape Counter({1: 235, 0: 143})\n",
            "\n",
            "Both\n",
            "NeighbourhoodCleaningRule shape Counter({1: 235, 0: 137})\n",
            "OneSidedSelection shape Counter({1: 235, 0: 230})\n",
            "\n",
            "ClusterCentroids shape Counter({0: 235, 1: 235})\n",
            "\n",
            "(495, 11)\n",
            "0.0    260\n",
            "1.0    235\n",
            "Name: num, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S__n2P-hIbOu"
      },
      "source": [
        "#### Into DataLoader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lm5hX-ctGJ55"
      },
      "source": [
        "pdXtr = Xtr\n",
        "pdYtr = Ytr\n",
        "pdXva = Xva\n",
        "pdYva = Yva\n",
        "\n",
        "npXtr = np.array(pdXtr, dtype='float')\n",
        "npYtr = np.array(pdYtr, dtype='float')\n",
        "npXva = np.array(pdXva, dtype='float')\n",
        "npYva = np.array(pdYva, dtype='float')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UABYWsC9Og68",
        "outputId": "7b850dee-28b3-4e98-af2e-185079a64651"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "tensorXtr = torch.Tensor(npXtr) # transform to torch tensor\n",
        "tensorYtr = torch.Tensor(npYtr)\n",
        "tensorYtr = tensorYtr.type(torch.LongTensor)\n",
        "datasetTr = TensorDataset(tensorXtr, tensorYtr) # create your datset\n",
        "trainloader = DataLoader(datasetTr, batch_size, True) # create your dataloader\n",
        "\n",
        "tensorXva = torch.Tensor(npXva) # transform to torch tensor\n",
        "tensorYva = torch.Tensor(npYva)\n",
        "tensorYva = tensorYva.type(torch.LongTensor)\n",
        "datasetVa = TensorDataset(tensorXva, tensorYva) # create your datset\n",
        "valiloader = DataLoader(datasetVa, batch_size, True) # create your dataloader\n",
        "\n",
        "data, labels = next(iter(trainloader))\n",
        "print(data.shape, labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([30, 10]) torch.Size([30])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWnT1hkYwLfe"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL38510fwNuM"
      },
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) #input and first hidden layer\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size) #hidden layer\n",
        "        self.fc3 = nn.Linear(hidden_size, hidden_size) #hidden layer\n",
        "        self.fc4 = nn.Linear(hidden_size, num_classes) #output\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc3(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc4(out)\n",
        "        return out\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_PVEzmzwX5a"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X_v8B6_WwZpv"
      },
      "source": [
        "# Train the model\n",
        "total_step = len(trainloader)\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (data, target) in enumerate(trainloader):\n",
        "    # Move tensors to the configured device\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(data)\n",
        "    loss = criterion(outputs, target)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 5 == 0 and (i+1) % 4 == 0:\n",
        "      print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3sdz3w9Ud1U"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVOtxHokUvfS",
        "outputId": "16fead32-ae2c-4ba4-c837-c8e2989d2b8f"
      },
      "source": [
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data, labels in valiloader:\n",
        "        data = data.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        #print(\"Predicted:\", predicted, \"\\nLabel:\", labels)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the test daata: {} %'.format(100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Accuracy of the network on the test daata: 78.91566265060241 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qck-MI05cgft"
      },
      "source": [
        "#!cat /content/drive/My\\ Drive/184A/finalProj/data/processed.cleveland.txt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}