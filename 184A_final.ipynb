{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BP7uv58DUm2A"
      },
      "source": [
        "# This only uses the cleveland datatset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YF4DKdY2gO_Z"
      },
      "source": [
        "### Setup"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dy1maEvif5p8"
      },
      "source": [
        "import pandas as pd\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "%matplotlib inline"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZqYe6Xlzb8oq",
        "outputId": "c7db503c-58c1-497d-d14e-4cbc6db8f556"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0GF9FW1dgWeR"
      },
      "source": [
        "### Check GPU Avaliability"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dYs0N24uggfO",
        "outputId": "21960339-73a3-4bad-b98f-16cb19946f98"
      },
      "source": [
        "# Device configuration\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cpu\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "egeCL0gaQq4a"
      },
      "source": [
        "### Hyper-parameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0XAaYvauwSic"
      },
      "source": [
        "# Hyper-parameters\n",
        "input_size = 13\n",
        "hidden_size = 10\n",
        "num_classes = 5\n",
        "\n",
        "num_epochs = 100\n",
        "batch_size = 30\n",
        "learning_rate = 0.001"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xJH59-U7wyDv"
      },
      "source": [
        "### Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KI1N737UycUY"
      },
      "source": [
        "#pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_8yxmoaIx7Be"
      },
      "source": [
        "#import pandas_profiling as pp\n",
        "#import warnings\n",
        "#warnings.filterwarnings('ignore')\n",
        "#pp.ProfileReport(dataset, title = 'Pandas Profiling report of \"dataset\"', html = {'style':{'full_width': True}})"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PavBkWpxfUo2"
      },
      "source": [
        "#load and clean dataset\n",
        "dataset1 = pd.read_csv('/content/drive/My Drive/184A/finalProj/data/processed.cleveland.txt', header = None)\n",
        "dataset1.columns = ['age','sex','cp','trestbps','chol','fbs','restecg','thalach','exang','oldpeak','slope','ca','thal','num']\n",
        "dataset1 = dataset1.replace('?', np.nan)\n",
        "dataset1 = dataset1.astype({'age': 'float64','sex': 'category','cp': 'category','trestbps': 'float64','chol': 'float64', 'fbs': 'category', 'restecg': 'category', 'thalach': 'float64', 'exang': 'category', 'oldpeak': 'float64', 'slope': 'category', 'ca': 'category', 'thal': 'category', 'num': 'category'})\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ndcyGddZVNdu"
      },
      "source": [
        "from sklearn.utils import shuffle\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "np.random.seed(0)\n",
        "dataset1 = shuffle(dataset1)\n",
        "sc = StandardScaler()\n",
        "dataset1.iloc[:,0:-1] = sc.fit_transform(dataset1.iloc[:,0:-1])\n",
        "\n",
        "def changeTarget(value):\n",
        "  return 1 if value > 0 else 0\n",
        "#dataset1['num'] = dataset1['num'].apply(changeTarget) #to make it just 0 and 1's\n",
        "\n",
        "datasetTrain = dataset1.iloc[0:int(0.75*dataset1.shape[0]), :]\n",
        "datasetVali = dataset1.iloc[int(0.75*dataset1.shape[0]):, :]\n",
        "\n",
        "pdXtr = datasetTrain.iloc[:,0:-1]\n",
        "pdYtr = datasetTrain.iloc[:,-1]\n",
        "pdXva = datasetVali.iloc[:,0:-1]\n",
        "pdYva = datasetVali.iloc[:,-1]\n",
        "\n",
        "npXtr = np.array(pdXtr, dtype='float')\n",
        "npYtr = np.array(pdYtr, dtype='float')\n",
        "npXva = np.array(pdXva, dtype='float')\n",
        "npYva = np.array(pdYva, dtype='float')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UABYWsC9Og68",
        "outputId": "a507f7ae-cf6d-4236-a34f-9bf8e79ff9f6"
      },
      "source": [
        "from torch.utils.data import TensorDataset, DataLoader\n",
        "\n",
        "tensorXtr = torch.Tensor(npXtr) # transform to torch tensor\n",
        "tensorYtr = torch.Tensor(npYtr)\n",
        "tensorYtr = tensorYtr.type(torch.LongTensor)\n",
        "datasetTr = TensorDataset(tensorXtr, tensorYtr) # create your datset\n",
        "trainloader = DataLoader(datasetTr, batch_size, True) # create your dataloader\n",
        "\n",
        "tensorXva = torch.Tensor(npXva) # transform to torch tensor\n",
        "tensorYva = torch.Tensor(npYva)\n",
        "tensorYva = tensorYva.type(torch.LongTensor)\n",
        "datasetVa = TensorDataset(tensorXva, tensorYva) # create your datset\n",
        "valiloader = DataLoader(datasetVa, batch_size, True) # create your dataloader\n",
        "\n",
        "data, labels = next(iter(trainloader))\n",
        "print(data.shape, labels.shape)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "torch.Size([30, 13]) torch.Size([30])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oWnT1hkYwLfe"
      },
      "source": [
        "### Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mL38510fwNuM"
      },
      "source": [
        "class NeuralNet(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(NeuralNet, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) #input and first hidden layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc2 = nn.Linear(hidden_size, hidden_size) #second hidden layer\n",
        "        self.relu = nn.ReLU()\n",
        "        self.fc3 = nn.Linear(hidden_size, num_classes) #third hidden layer\n",
        "        #self.relu = nn.ReLU()\n",
        "        #self.fc4 = nn.Linear(hidden_size, num_classes)  #output\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        out = self.relu(out)\n",
        "        out = self.fc3(out)\n",
        "        #out = self.relu(out)\n",
        "        #out = self.fc4(out)\n",
        "        return out\n",
        "\n",
        "model = NeuralNet(input_size, hidden_size, num_classes).to(device)\n",
        "\n",
        "# Loss and optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "J_PVEzmzwX5a"
      },
      "source": [
        "### Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X_v8B6_WwZpv",
        "outputId": "53aba2a0-5021-41a7-b82e-1543ca8669f5"
      },
      "source": [
        "# Train the model\n",
        "total_step = len(trainloader)\n",
        "for epoch in range(num_epochs):\n",
        "  for i, (data, target) in enumerate(trainloader):\n",
        "    # Move tensors to the configured device\n",
        "    data = data.to(device)\n",
        "    target = target.to(device)\n",
        "\n",
        "    # Forward pass\n",
        "    outputs = model(data)\n",
        "    loss = criterion(outputs, target)\n",
        "\n",
        "    # Backward and optimize\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if (epoch+1) % 25 == 0 and (i+1) % 4 == 0:\n",
        "      print('Epoch [{}/{}], Step [{}/{}], Loss: {:.4f}'.format(epoch+1, num_epochs, i+1, total_step, loss.item()))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch [25/100], Step [4/8], Loss: 0.9997\n",
            "Epoch [25/100], Step [8/8], Loss: 1.0735\n",
            "Epoch [50/100], Step [4/8], Loss: 0.9458\n",
            "Epoch [50/100], Step [8/8], Loss: 0.9359\n",
            "Epoch [75/100], Step [4/8], Loss: 0.8403\n",
            "Epoch [75/100], Step [8/8], Loss: 0.7460\n",
            "Epoch [100/100], Step [4/8], Loss: 0.7040\n",
            "Epoch [100/100], Step [8/8], Loss: 1.2039\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rd5Hueb2NbZT"
      },
      "source": [],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a3sdz3w9Ud1U"
      },
      "source": [
        "### Test"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VVOtxHokUvfS",
        "outputId": "d9feb483-3723-433b-c9bf-22373baf13ac"
      },
      "source": [
        "# Test the model\n",
        "# In test phase, we don't need to compute gradients (for memory efficiency)\n",
        "with torch.no_grad():\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    for data, labels in valiloader:\n",
        "        data = data.to(device)\n",
        "        labels = labels.to(device)\n",
        "        outputs = model(data)\n",
        "        _, predicted = torch.max(outputs.data, 1)\n",
        "        print(\"Predicted:\", predicted, \"\\nLabel:\", labels)\n",
        "        total += labels.size(0)\n",
        "        correct += (predicted == labels).sum().item()\n",
        "\n",
        "    print('Accuracy of the network on the test daata: {} %'.format(100 * correct / total))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Predicted: tensor([3, 0, 1, 3, 3, 0, 3, 0, 0, 1, 0, 1, 0, 1, 0, 3, 1, 0, 0, 1, 1, 0, 0, 0,\n",
            "        1, 0, 3, 2, 0, 0]) \n",
            "Label: tensor([0, 1, 2, 3, 3, 0, 4, 0, 0, 2, 0, 3, 2, 0, 0, 2, 1, 0, 0, 0, 1, 0, 0, 0,\n",
            "        2, 0, 4, 0, 2, 0])\n",
            "Predicted: tensor([0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 3, 0, 0, 0, 0, 0,\n",
            "        3, 3, 0, 0, 3, 0]) \n",
            "Label: tensor([0, 0, 0, 0, 0, 0, 3, 0, 0, 1, 0, 0, 0, 0, 0, 1, 0, 3, 1, 1, 0, 0, 0, 0,\n",
            "        3, 2, 0, 0, 2, 2])\n",
            "Predicted: tensor([0, 3, 0, 0, 3, 0, 3, 0, 0, 2, 3, 0, 0, 0, 3]) \n",
            "Label: tensor([0, 3, 1, 0, 0, 0, 1, 0, 0, 2, 1, 0, 0, 0, 4])\n",
            "Accuracy of the network on the test daata: 64.0 %\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qck-MI05cgft"
      },
      "source": [
        "#with open('/content/drive/My Drive/184A/finalProj/data/processed.cleveland.txt', 'r') as f:\n",
        "#  f.write('Hello Google Drive!')\n",
        "#!cat /content/drive/My\\ Drive/184A/finalProj/data/processed.cleveland.txt"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}